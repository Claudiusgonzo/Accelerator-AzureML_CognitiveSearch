{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Aggregate Knowledge Store Documents\n",
    "\n",
    "The projected documents to the knowledge store only contain the sentences with a label. This notebook will aggregate all the sentences form the different documents into a single document that will be loaded into AML as a dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "from pandas.io.json import json_normalize \n",
    "import os\n",
    "import base64\n",
    "from posixpath import basename, dirname\n",
    "from urllib.parse import urlparse\n",
    "from azure.storage.blob import BlockBlobService\n",
    "import sys"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Storage container for Knowledge store is \"labeled-data\". This is defined in the knowledgeStore skill. This script reads the data from \"labeled-data\" and generate a labeled data set for training. You can configure how many total sentences you need using total_sentences variable mentioned below. sentences_per_file defines maximum sentences per file. If both are the same then this will script will generate single file. Otherwise it will generate multiple files which needs to be aggregated. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "azure_storage_account_name = 'XXXXXXXXXXXXX' # Knowledge store -- same as storage acct\n",
    "azure_storage_account_key = 'XXXXXXXXXXXXX'\n",
    "sentences_per_file=100000\n",
    "total_sentences=100000\n",
    "\n",
    "if azure_storage_account_name is None or azure_storage_account_key is None:\n",
    "    raise Exception(\"Provide your specific name and key for your Azure Storage account--see the Prerequisites section earlier.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_to_csv( file_name):\n",
    "    with open(file_name, encoding=\"utf8\") as data_file:    \n",
    "        data = json.load(data_file)  \n",
    "    \n",
    "    df = json_normalize(data, ['labeled-data', 'annotations'], [['labeled-data','sentence_count'], 'document_id'])\n",
    "    if(df.empty):\n",
    "        return (file_name, 0)\n",
    "    else:\n",
    "        \n",
    "        df = df.rename(columns={\"token\": \"Word\", \"label\": \"Tag\"})\n",
    "        df['labeled-data.sentence_count'] = df['labeled-data.sentence_count'].astype(str)\n",
    "        df['sentence'] = df[['document_id', 'labeled-data.sentence_count']].agg('-'.join, axis=1)\n",
    "        df.drop(['labeled-data.sentence_count', 'document_id'], axis=1, inplace=True)\n",
    "        lines = df.groupby('sentence')['sentence'].nunique().count()\n",
    "    return (df, lines)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_file(df, file_path, file_name, file_num,blob_service,labelleddata_container):\n",
    "    df.to_csv(file_path, mode='a', index=False)\n",
    "    blob_service.create_blob_from_path(labelleddata_container, file_name+str(file_num)+\".csv\", file_path)\n",
    "    os.remove(file_path)\n",
    "    \n",
    "    df[\"line\"]=df[\"Word\"]+\" \" +df[\"Tag\"]\n",
    "    df.loc[(df[\"POS\"]== \".\"),\"line\" ]=\". O\\n\"\n",
    "    with open(file_path, \"w\") as f:\n",
    "        f.write(\"\\n\".join(list(df[\"line\"])))\n",
    "    blob_service.create_blob_from_path(labelleddata_container, file_name+str(file_num)+\".txt\", file_path)\n",
    "    os.remove(file_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_name = \"ner_dataset\" \n",
    "file_path=\"TempData\"\n",
    "labelleddata_container='labeled-data-df'\n",
    "ks_container='labeled-data'\n",
    "\n",
    "blob_service = BlockBlobService(azure_storage_account_name, azure_storage_account_key)\n",
    "container_status = blob_service.create_container(labelleddata_container)\n",
    "blobs = blob_service.list_blobs(ks_container)\n",
    "sentences = 0\n",
    "docs = 0\n",
    "fileNum=1\n",
    "linesNum=0\n",
    "aggDF=pd.DataFrame()\n",
    "sentenceID=0\n",
    "\n",
    "for blob in blobs:\n",
    "    docs = docs + 1\n",
    "    sentenceVal=None\n",
    "    # Read blob to a temp file and use pandas to convert to shape needed to train the model\n",
    "    #print(\"blob.name---\",blob.name)\n",
    "    blob_service.get_blob_to_path(ks_container, blob.name, 'sample.json')\n",
    "    (df, lines) = transform_to_csv('sample.json')\n",
    "\n",
    "    if(lines!=0):\n",
    "        for i, row in df.iterrows():\n",
    "            if(df.at[i,'sentence']!=sentenceVal):\n",
    "                sentenceID += 1                \n",
    "                sentenceVal=df.at[i,'sentence']\n",
    "                df.at[i,'sentence'] = sentenceID\n",
    "            else:\n",
    "                df.at[i,'sentence'] = sentenceID        \n",
    "        linesNum += lines\n",
    "        aggDF=pd.concat([aggDF, df])\n",
    "        if(linesNum>sentences_per_file):\n",
    "            save_file(aggDF, file_path, file_name, fileNum,blob_service,labelleddata_container)\n",
    "            print(f'Succesfully generated labeled data with {sentences} sentences from {docs} documents')\n",
    "            fileNum=fileNum+1\n",
    "            aggDF=pd.DataFrame()\n",
    "            linesNum=0\n",
    "\n",
    "        sentences += lines\n",
    "        if (sentences > total_sentences ): # 100000 samples should suffice to train the model\n",
    "            if(total_sentences >sentences_per_file):\n",
    "                save_file(aggDF, file_path, file_name, fileNum,blob_service,labelleddata_container)\n",
    "                print(f'Succesfully generated labeled data with {sentences} sentences from {docs} documents')\n",
    "            sys.exit(\"quitting as total sentences collected  so far exceeds the limit\")    \n",
    "if ( (sentences <sentences_per_file) or (linesNum<sentences_per_file and sentences>sentences_per_file)):\n",
    "    save_file(aggDF, file_path, file_name, fileNum,blob_service,labelleddata_container)\n",
    "\n",
    "\n",
    "print(f'Succesfully generated labeled data with {sentences} sentences from {docs} documents')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### You now have a labeled dataset!\n",
    "\n",
    "#### Validate Results\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Check your storage account, you should now have a new container ```labeled-data-df``` with files named ```ner_dataset1.csv``` and ```ner_dataset1.txt```. You might have more than these, if you decided to split the files.  These files have all the sentences with labels from all the files processed. If you have configured the script to generate multiple txt files, download all of them and aggregate them into one. \n",
    "CSV file has POS,Tag,Word and sentence columns. Text file has Word and Tag columns. \n",
    "\n",
    "For the next step, download the ```ner_dataset1.txt``` file. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
