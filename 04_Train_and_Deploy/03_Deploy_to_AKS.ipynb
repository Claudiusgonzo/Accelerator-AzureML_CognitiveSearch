{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copyright (c) Microsoft Corporation. All rights reserved.\n",
    "\n",
    "Licensed under the MIT License."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deploy Model to AKS\n",
    "\n",
    "Estimated time to complete: 30-60 minutes\n",
    "\n",
    "Once you've tested the model and are satisfied with the results, deploy the model as a web service hosted in [Azure Kubernetes Service cluster](https://docs.microsoft.com/en-us/azure/machine-learning/how-to-deploy-azure-kubernetes-service).\n",
    "\n",
    "To deploy the model to AKS, we need the following:\n",
    "\n",
    "1. A scoring script to show how to use the model\n",
    "2. An environment file to show what packages need to be installed\n",
    "3. Deployment configuration for the ML service deployed on AKS\n",
    "4. The model you trained before\n",
    "\n",
    "\n",
    "### Prerequisites\n",
    "You need to complete the steps in 01_Prerequisites and 02_Train_Model NoteBooks. Note that some of the code is duplicated between ACI and AKS deployment Notebooks. This is to ensure that entire code for AKS or ACI deployment is in one Notebook. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.0 Connect to workspace"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialize a Workspace object from the existing workspace you created in the Prerequisites step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core import Workspace\n",
    "\n",
    "try:\n",
    "    ws = Workspace.from_config()\n",
    "    print(ws.name, ws.location, ws.resource_group, ws.location, sep='\\t')\n",
    "    print('Library configuration succeeded')\n",
    "except:\n",
    "    print('Workspace not found')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.0 Create scoring script\n",
    "Create the scoring script, called score.py, used by the web service call to show how to use the model.\n",
    "\n",
    "You must include two required functions into the scoring script:\n",
    "\n",
    "The init() function, which typically loads the model into a global object. This function is run only once when the Docker container is started.\n",
    "\n",
    "The run(input_data) function uses the model to predict a value based on the input data. Inputs and outputs to the run typically use JSON for serialization and de-serialization, but other formats are supported.\n",
    "\n",
    "Input data from KM will be in the following JSON format\n",
    "\n",
    "```json\n",
    "{\n",
    "\t\"raw_data\": {\n",
    "\t\t\"text\": \"The Bing News Search API makes it easy to integrate Bing's cognitive news searching capabilities into your applications. If your Cosmos DB account is used by other Azure services like Azure Cognitive Search , or is accessed from Stream analytics or Power BI , you allow access by selecting Accept connections from within global Azure datacenters . \"\n",
    "\t}\n",
    "}\n",
    "```\n",
    "\n",
    "\n",
    "Output data from ML servcie should be in the following fomat\n",
    "```json\n",
    "{\n",
    "\t\"tags\": {\n",
    "\t\t\"products\": [\n",
    "\t\t\t\"Bing News Search\",\n",
    "\t\t\t\"Cosmos DB\",\n",
    "\t\t\t\"Cognitive Search\",\n",
    "\t\t\t\"Stream analytics\"\n",
    "\t\t],\n",
    "\t\t\"features\": []\n",
    "\t}\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open('labelfile.txt', 'r') as fp:\n",
    "    labelmap = json.load(fp)\n",
    "labelmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score_text= '''\n",
    "\n",
    "import json\n",
    "import argparse\n",
    "import os\n",
    "import random\n",
    "import sys\n",
    "from tempfile import TemporaryDirectory\n",
    "from azureml.core import Dataset, Run\n",
    "import pandas as pd\n",
    "import torch\n",
    "\n",
    "# Inference schema for schema discovery\n",
    "from inference_schema.schema_decorators import input_schema, output_schema\n",
    "from inference_schema.parameter_types.numpy_parameter_type import NumpyParameterType\n",
    "from inference_schema.parameter_types.standard_py_parameter_type import StandardPythonParameterType\n",
    "\n",
    "from utils_nlp.common.pytorch_utils import dataloader_from_dataset\n",
    "from utils_nlp.common.timer import Timer\n",
    "from utils_nlp.dataset.ner_utils import preprocess_conll\n",
    "from utils_nlp.models.transformers.named_entity_recognition import (\n",
    "    TokenClassificationProcessor, TokenClassifier)\n",
    "\n",
    "from azureml.core.model import Model\n",
    "\n",
    "model_name = \"bert-base-cased\"\n",
    "DO_LOWER_CASE = False\n",
    "TRAILING_PIECE_TAG = \"X\"\n",
    "DEVICE = \"cuda\"\n",
    "test_fraction = 0.2\n",
    "train_file = \"ner_dataset2\"\n",
    "max_len = 256\n",
    "CACHE_DIR = \"./temp\"\n",
    "label_map=  %s\n",
    "BATCH_SIZE = 5\n",
    "device = \"cpu\"\n",
    "\n",
    "\n",
    "def init():\n",
    "    global model\n",
    "        \n",
    "    # load the pretrained model\n",
    "    model = TokenClassifier(model_name=model_name, num_labels=len(label_map), cache_dir=CACHE_DIR )\n",
    "    # Load the fine tuned weights\n",
    "    model_path = Model.get_model_path('bertkm_ner')\n",
    "    # apply the fine tuned weights to pretrained model\n",
    "    model.model.load_state_dict(torch.load(model_path, map_location=device))\n",
    "\n",
    "# Inference schema for schema discovery\n",
    "standard_sample_input = {'text': 'a sample input record containing some text' }\n",
    "standard_sample_output = {'tags': {'products': ['Cognitive Search', 'Cosmos DB'], \"features\": ['indexer']}}\n",
    "\n",
    "@input_schema('raw_data', StandardPythonParameterType(standard_sample_input))\n",
    "@output_schema(StandardPythonParameterType(standard_sample_output))\n",
    "def run(raw_data):\n",
    "    input_txt = \"\"\n",
    "    try:\n",
    "        input_txt = raw_data[\"text\"]\n",
    "        tag_list = []\n",
    "        processor = TokenClassificationProcessor(model_name=model_name, to_lower=DO_LOWER_CASE, cache_dir=CACHE_DIR)\n",
    "\n",
    "\n",
    "        product=False\n",
    "        feature=False\n",
    "        product_temp=None \n",
    "        feature_temp=None\n",
    "\n",
    "        input_tokens = input_txt.split() \n",
    "\n",
    "        sample_dataset = processor.preprocess_for_bert(\n",
    "            text=[input_tokens],\n",
    "            max_len=max_len,\n",
    "            labels=None,\n",
    "            label_map=label_map,\n",
    "            trailing_piece_tag=TRAILING_PIECE_TAG,\n",
    "        )\n",
    "        sample_dataloader = dataloader_from_dataset(\n",
    "            sample_dataset, batch_size=BATCH_SIZE, num_gpus=None, shuffle=False, distributed=False\n",
    "        )\n",
    "        #for AKS deployment remove the Verbose flag\n",
    "        preds = model.predict(\n",
    "                test_dataloader=sample_dataloader,\n",
    "                num_gpus=None,\n",
    "                verbose=False\n",
    "            )\n",
    "        tags_predicted = model.get_predicted_token_labels(\n",
    "            predictions=preds,\n",
    "            label_map=label_map,\n",
    "            dataset=sample_dataset\n",
    "        )\n",
    "        \n",
    "        tags = {\"products\": [],\"features\": []}\n",
    "        loc = 0\n",
    "        product_temp=\"\"\n",
    "        feature_temp=\"\"    \n",
    "        for i in input_tokens:\n",
    "            if(loc<256 and loc < len(tags_predicted[0])):\n",
    "                if tags_predicted[0][loc] == 'B-Product':\n",
    "                    product = True\n",
    "                    product_temp=i\n",
    "                elif tags_predicted[0][loc] == 'I-Product':                \n",
    "                    product_temp += \" \" +i\n",
    "                elif tags_predicted[0][loc] == 'B-Feature':\n",
    "                    feature = True\n",
    "                    feature_temp=i\n",
    "                elif tags_predicted[0][loc] == 'I-Feature':\n",
    "                    feature_temp += \" \" +i            \n",
    "                else:\n",
    "                    if(product==True):\n",
    "                        tags[\"products\"].append(product_temp)\n",
    "                        product=False\n",
    "                    elif(feature==True):\n",
    "                        tags[\"features\"].append(feature_temp)\n",
    "                        feature=False                    \n",
    "                loc = loc+1\n",
    "\n",
    "\n",
    "\n",
    "        output = {\"tags\": tags}  \n",
    "\n",
    "        return(output)\n",
    "    except Exception as e:\n",
    "        result = str(e)\n",
    "        # return error message back to the client\n",
    "        return json.dumps({\"error\": result})\n",
    "    \n",
    "'''%str(labelmap)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is a way to debug Scoring script locally. Refer to the script 04_Debug_Score_Script.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"score.py\", \"w\") as stream:\n",
    "   stream.write(score_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.0 Deploy Model to AKS\n",
    "Next we will go through the steps to deploy the model in AKS.\n",
    "\n",
    "### 3.1 Create a custom environment\n",
    "Specify the model's runtime environment by creating an Environment object and providing the CondaDependencies needed by your model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PIP_PACKAGES = [\"azureml-defaults\", \"azureml-monitoring\", \"seqeval[gpu]\", \"torch==1.4\", \"tqdm==4.31.1\", \"transformers==2.8.0\", \"nltk==3.5\", \"azureml-sdk==1.3.0\", \"inference-schema\"]\n",
    "CONDA_PACKAGES = [\"numpy\", \"scikit-learn\", \"pandas\"]\n",
    "utils_nlp_file=\"./nlp-recipes-utils/utils_nlp-2.0.0-py3-none-any.whl\"\n",
    "PYTHON_VERSION = \"3.6.8\"\n",
    "USE_GPU = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# conda env setup\n",
    "from azureml.core.conda_dependencies import CondaDependencies\n",
    "from azureml.core import ScriptRunConfig\n",
    "from azureml.core.environment import Environment, DEFAULT_GPU_IMAGE\n",
    "\n",
    "myenv = Environment(name=\"myenv\")\n",
    "\n",
    "conda_dependencies = CondaDependencies.create(\n",
    "    conda_packages=CONDA_PACKAGES,\n",
    "    pip_packages=PIP_PACKAGES,\n",
    "    python_version=PYTHON_VERSION,\n",
    ")\n",
    "\n",
    "nlp_repo_whl = Environment.add_private_pip_wheel(\n",
    "    workspace=ws,\n",
    "    file_path=utils_nlp_file,\n",
    "    exist_ok=True,\n",
    ")\n",
    "#we can also add using the approach mentioned at https://docs.microsoft.com/en-us/azure/machine-learning/how-to-use-environments#add-packages-to-an-environment\n",
    "\n",
    "conda_dependencies.add_pip_package(nlp_repo_whl)\n",
    "\n",
    "# Adds dependencies to PythonSection of myenv\n",
    "myenv.python.conda_dependencies=conda_dependencies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Create the InferenceConfig\n",
    "Create the inference config that will be used when deploying the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core.model import InferenceConfig\n",
    "inf_config = InferenceConfig(entry_script='score.py', environment=myenv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Provision AKS Cluster\n",
    "This is a one time setup. You can reuse this cluster for multiple deployments after it has been created. If you delete the cluster or the resource group that contains it, then you would have to recreate it.\n",
    "Here we are creating AKS Cluster which is SSL enabled with certificate from Microsoft. If you need to enable SSL with your own certificate, follow the steps mentioned [here](https://docs.microsoft.com/en-us/azure/machine-learning/service/how-to-secure-web-service#enable).\n",
    "\n",
    "To get more details about creating a new AKS cluster refer this [link](https://docs.microsoft.com/en-us/azure/machine-learning/service/how-to-deploy-azure-kubernetes-service#create-a-new-aks-cluster).\n",
    "\n",
    "This takes about 20 minutes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core.compute import AksCompute, ComputeTarget\n",
    "# Config used to create a new AKS cluster and enable SSL\n",
    "prov_config = AksCompute.provisioning_configuration()\n",
    "\n",
    "# Leaf domain label generates a name using the formula\n",
    "#  \"<leaf-domain-label>######.<azure-region>.cloudapp.azure.net\"\n",
    "#  where \"######\" is a random series of characters\n",
    "prov_config.enable_ssl(leaf_domain_label = \"contoso\")\n",
    "\n",
    "aks_name = 'kmaml-aks' \n",
    "\n",
    "#Use existing clusters\n",
    "#aks_target = ComputeTarget(ws, 'kmaml-aks')\n",
    "\n",
    "# Create the cluster   \n",
    "aks_target = ComputeTarget.create(workspace = ws, \n",
    "                                  name = aks_name, \n",
    "                                   provisioning_configuration = prov_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wait for the create process to complete\n",
    "aks_target.wait_for_completion(show_output = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 Create configuration file\n",
    "Create a deployment configuration file and specify the number of CPUs and gigabyte of RAM needed. While it depends on your model, the default of 1 core and 1 gigabyte of RAM is usually sufficient for many models. If you feel you need more later, you would have to recreate the image and redeploy the service.`\n",
    "\n",
    "It takes 5-10  mins to deploy the service."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "from azureml.core.model import Model\n",
    "from azureml.core.webservice import Webservice\n",
    "from azureml.core.image import ContainerImage\n",
    "from azureml.core.webservice import AksWebservice, Webservice\n",
    "from azureml.core.compute import AksCompute, ComputeTarget\n",
    "\n",
    "\n",
    "# If deploying to a cluster configured for dev/test, ensure that it was created with enough\n",
    "# cores and memory to handle this deployment configuration. Note that memory is also used by\n",
    "# things such as dependencies and AML components.\n",
    "\n",
    "aks_config = AksWebservice.deploy_configuration(autoscale_enabled=True, \n",
    "                                                       autoscale_min_replicas=1, \n",
    "                                                       autoscale_max_replicas=3, \n",
    "                                                       autoscale_refresh_seconds=10, \n",
    "                                                       autoscale_target_utilization=70,\n",
    "                                                       auth_enabled=True, \n",
    "                                                       cpu_cores=1, memory_gb=2, \n",
    "                                                       scoring_timeout_ms=5000, \n",
    "                                                       replica_max_concurrent_requests=2, \n",
    "                                                       max_request_wait_time=5000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.5 Deploy to AKS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "service_name = 'kmaml-aks'\n",
    "model=Model(ws, 'bertkm_ner')\n",
    "\n",
    "aks_service = Model.deploy(workspace=ws,\n",
    "                           name=service_name,\n",
    "                           models=[model],\n",
    "                           inference_config=inf_config,\n",
    "                           deployment_config=aks_config,\n",
    "                           deployment_target=aks_target,\n",
    "                           overwrite = True)\n",
    "\n",
    "aks_service.wait_for_deployment(show_output = True)\n",
    "print(aks_service.state)\n",
    "print(aks_service.get_logs())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "primary, secondary = aks_service.get_keys()\n",
    "print(primary)\n",
    "print(aks_service.state)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the scoring web service's HTTP endpoint, which accepts REST client calls. We will test the ML web service using this REST API endpoint.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(aks_service.scoring_uri)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.0 Test deployed service in AKS\n",
    "We test the web sevice by passing data. Run() method retrieves API keys behind the scenes to make sure that call is authenticated.\n",
    "\n",
    "Note that in the following example the model identifies Bing News Search as a Product. This entity was not part of the Tarining data. That is the power of BERT model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "\n",
    "import time\n",
    "\n",
    "\n",
    "# send a random row from the test set to score\n",
    "input_data = \"\"\"{\"raw_data\": {\"text\": \"The Bing News Search API makes it easy to integrate Bing's cognitive news searching capabilities into your applications. If your Cosmos DB account is used by other Azure services like Azure Cognitive Search , or is accessed from Stream analytics or Power BI , you allow access by selecting Accept connections from within global Azure datacenters . \"}}\"\"\"\n",
    "\n",
    "\n",
    "# for AKS deployment that is key auth enabled, you'd need to get the service key in the header as well\n",
    "primary, secondary = aks_service.get_keys()\n",
    "\n",
    "# Set the content type\n",
    "headers = {'Content-Type': 'application/json'}\n",
    "# If authentication is enabled, set the authorization header\n",
    "headers['Authorization'] = f'Bearer {primary}'\n",
    "\n",
    "# Make the request and display the response\n",
    "t0 = time.ctime(time.time())\n",
    "print(\"t0:\", t0)\n",
    "\n",
    "resp = requests.post(aks_service.scoring_uri, input_data, headers=headers)\n",
    "print(resp.text)\n",
    "t1 = time.ctime(time.time())\n",
    "print(\"t1:\", t1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next Step\n",
    "\n",
    "Now that we have deployed ML model as a servcie in AKS, we need to integrate that with Cognitive Search. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
