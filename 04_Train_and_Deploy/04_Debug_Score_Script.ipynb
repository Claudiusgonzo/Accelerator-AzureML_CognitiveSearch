{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copyright (c) Microsoft Corporation. All rights reserved.\n",
    "\n",
    "Licensed under the MIT License."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Debugging Score Script locally\n",
    "\n",
    "In order to debug and analyze the output of Score script, we need to deploy the model(either locally or to ACI) and then review the prediction from the ML service. This can be time consuming. There is a way to debug Score Script locally without deploying the model. In order to do this we need to the following \n",
    "\n",
    "1. Update the Score Script: Add code to the Score script so that the code gets executed when score script run as main program.\n",
    "\n",
    "2. Download the model and label map\n",
    "\n",
    "Thanks to Sam Kemp for providing this very useful tip!\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Install these packages to run the model locally\n",
    "!pip install transformers==2.8.0\n",
    "!pip install inference-schema"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.0 Connect to workspace"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialize a Workspace object from the existing workspace you created in the Prerequisites step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core import Workspace\n",
    "\n",
    "try:\n",
    "    ws = Workspace.from_config()\n",
    "    print(ws.name, ws.location, ws.resource_group, ws.location, sep='\\t')\n",
    "    print('Library configuration succeeded')\n",
    "except:\n",
    "    print('Workspace not found')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.0 Update scoring script\n",
    "\n",
    "\n",
    "Add code to the Score script so that the code gets executed when score script run as main program. This essentially means we need to add code block for \"if __name__ == \"__main__\":\" as shown below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.0 Download the model\n",
    "\n",
    "To debug the Score script locally, download the model file as follows. This will create a folder structure azureml-models\\bertkm_ner. bertkm_ner folder will have the latest version of the model file.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core.model import Model\n",
    "\n",
    "Model.get_model_path('bertkm_ner', _workspace=ws)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Download the label map\n",
    "import json\n",
    "with open('labelfile.txt', 'r') as fp:\n",
    "    labelmap = json.load(fp)\n",
    "type(labelmap)\n",
    "labelmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score_text= '''\n",
    "\n",
    "import json\n",
    "import argparse\n",
    "import os\n",
    "import random\n",
    "import sys\n",
    "from tempfile import TemporaryDirectory\n",
    "from azureml.core import Dataset, Run\n",
    "import pandas as pd\n",
    "import torch\n",
    "\n",
    "# Inference schema for schema discovery\n",
    "from inference_schema.schema_decorators import input_schema, output_schema\n",
    "from inference_schema.parameter_types.numpy_parameter_type import NumpyParameterType\n",
    "from inference_schema.parameter_types.standard_py_parameter_type import StandardPythonParameterType\n",
    "\n",
    "from utils_nlp.common.pytorch_utils import dataloader_from_dataset\n",
    "from utils_nlp.common.timer import Timer\n",
    "from utils_nlp.dataset.ner_utils import preprocess_conll\n",
    "from utils_nlp.models.transformers.named_entity_recognition import (\n",
    "    TokenClassificationProcessor, TokenClassifier)\n",
    "\n",
    "from azureml.core.model import Model\n",
    "\n",
    "model_name = \"bert-base-cased\"\n",
    "DO_LOWER_CASE = False\n",
    "TRAILING_PIECE_TAG = \"X\"\n",
    "DEVICE = \"cuda\"\n",
    "test_fraction = 0.2\n",
    "train_file = \"ner_dataset2\"\n",
    "max_len = 256\n",
    "CACHE_DIR = \"./temp\"\n",
    "label_map=  %s\n",
    "BATCH_SIZE = 5\n",
    "device = \"cpu\"\n",
    "\n",
    "\n",
    "def init():\n",
    "    global model\n",
    "        \n",
    "    # load the pretrained model\n",
    "    model = TokenClassifier(model_name=model_name, num_labels=len(label_map), cache_dir=CACHE_DIR )\n",
    "    # Load the fine tuned weights\n",
    "    model_path = Model.get_model_path('bertkm_ner')\n",
    "    # apply the fine tuned weights to pretrained model\n",
    "    model.model.load_state_dict(torch.load(model_path, map_location=device))\n",
    "\n",
    "# Inference schema for schema discovery\n",
    "standard_sample_input = {'text': 'a sample input record containing some text' }\n",
    "standard_sample_output = {'tags': {'products': ['Cognitive Search', 'Cosmos DB'], \"features\": ['indexer']}}\n",
    "\n",
    "@input_schema('raw_data', StandardPythonParameterType(standard_sample_input))\n",
    "def run(raw_data):\n",
    "    input_txt = \"\"\n",
    "    try:\n",
    "        input_txt = raw_data[\"text\"]\n",
    "        tag_list = []\n",
    "        processor = TokenClassificationProcessor(model_name=model_name, to_lower=DO_LOWER_CASE, cache_dir=CACHE_DIR)\n",
    "\n",
    "\n",
    "        product=False\n",
    "        feature=False\n",
    "        product_temp=None \n",
    "        feature_temp=None\n",
    "\n",
    "        input_tokens = input_txt.split() \n",
    "\n",
    "        sample_dataset = processor.preprocess_for_bert(\n",
    "            text=[input_tokens],\n",
    "            max_len=max_len,\n",
    "            labels=None,\n",
    "            label_map=label_map,\n",
    "            trailing_piece_tag=TRAILING_PIECE_TAG,\n",
    "        )\n",
    "        sample_dataloader = dataloader_from_dataset(\n",
    "            sample_dataset, batch_size=BATCH_SIZE, num_gpus=None, shuffle=False, distributed=False\n",
    "        )\n",
    "        #for AKS deployment remove the Verbose flag\n",
    "        preds = model.predict(\n",
    "                test_dataloader=sample_dataloader,\n",
    "                num_gpus=None,\n",
    "                verbose=False\n",
    "            )\n",
    "        tags_predicted = model.get_predicted_token_labels(\n",
    "            predictions=preds,\n",
    "            label_map=label_map,\n",
    "            dataset=sample_dataset\n",
    "        )\n",
    "        \n",
    "        tags = {\"products\": [],\"features\": []}\n",
    "        loc = 0\n",
    "        product_temp=\"\"\n",
    "        feature_temp=\"\"    \n",
    "        for i in input_tokens:\n",
    "            if(loc<256 and loc < len(tags_predicted[0])):\n",
    "                if tags_predicted[0][loc] == 'B-Product':\n",
    "                    product = True\n",
    "                    product_temp=i\n",
    "                elif tags_predicted[0][loc] == 'I-Product':                \n",
    "                    product_temp += \" \" +i\n",
    "                elif tags_predicted[0][loc] == 'B-Feature':\n",
    "                    feature = True\n",
    "                    feature_temp=i\n",
    "                elif tags_predicted[0][loc] == 'I-Feature':\n",
    "                    feature_temp += \" \" +i            \n",
    "                else:\n",
    "                    if(product==True):\n",
    "                        tags[\"products\"].append(product_temp)\n",
    "                        product=False\n",
    "                    elif(feature==True):\n",
    "                        tags[\"features\"].append(feature_temp)\n",
    "                        feature=False                    \n",
    "                loc = loc+1\n",
    "\n",
    "\n",
    "\n",
    "        output = {\"tags\": tags}  \n",
    "\n",
    "        return(output)\n",
    "    except Exception as e:\n",
    "        result = str(e)\n",
    "        # return error message back to the client\n",
    "        return json.dumps({\"error\": result})\n",
    "        \n",
    "if __name__ == '__main__':\n",
    "    init()\n",
    "    \n",
    "    input_data = \"\"\"{\"raw_data\": {\"text\": \"If your Cosmos DB account is used by other Azure services like Azure Cognitive Search or the Bing Search API you will have good results.\"}}\"\"\"\n",
    "    input_json = json.loads(input_data)['raw_data']\n",
    "    resp=run(input_json ) \n",
    "    print(resp)\n",
    "    \n",
    "'''%str(labelmap)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"score.py\", \"w\") as stream:\n",
    "   stream.write(score_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.0 Run the score script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run score.py"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
